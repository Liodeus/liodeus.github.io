# AI cybersecurity review and testing
This is primarily based on the Databricks AI Security Framework (DASF). The goal is to provide a guide for conducting AI penetration testing from a pentester’s perspective, covering the entire process - from the initial client interaction to the specific tests that need to be performed.

To begin, I will focus on a black-box attacker perspective, with the possibility of incorporating some gray-box elements depending on the client's input.
## 1. Initial Phase: Client Consultation and Scoping
### 1.1 Client objectives and scope definition  
<TODO>

## Databricks AI security framework (DSAF)
### Model types
First, find out what type of AI models are being built or being used:
1. Predictive ML Models: These models focus on traditional machine learning techniques and are specifically designed for structured data. They are trained on your enterprise’s tabular datasets and are typically implemented in Python, often packaged in the MLflow format for deployment. Examples of predictive ML models include scikit-learn, XGBoost, PyTorch, and Hugging Face transformer models. Their primary use is to perform tasks such as regression and classification based on historical data.
2. State-of-the-Art Open Models: This category encompasses advanced foundation models that are designed for optimized inference on a variety of tasks, particularly those involving unstructured data. Examples include Meta Llama 3.1-405B-Instruct, BGE-Large, and Mixtral-8x7B, which are available for immediate use with pay-per-token pricing. These models can be deployed for workloads that require performance guarantees and can be fine-tuned for specific applications. Their usage patterns include Foundation Model APIs for large language models (LLMs), retrieval-augmented generation (RAG), pretraining, and fine-tuning, making them suitable for complex applications like natural language processing.
3. External Models (Third-Party Services): These models are hosted outside of your organization, typically provided by third-party vendors. Endpoints that serve external models can be centrally governed, allowing organizations to set rate limits and access controls. Examples include widely used foundation models such as OpenAI’s GPT-4 and Anthropic’s Claude. While these models provide easy access to advanced capabilities, they require reliance on the provider’s infrastructure and policies.

Summary of differences:
* Data type: Predictive ML Models focus on structured data, while State-of-the-Art Open Models are suited for unstructured data. External Models can cover both but are accessed via third-party services.
* Complexity: Predictive ML Models are generally simpler, targeting specific prediction tasks, whereas State-of-the-Art Open Models use advanced architectures for complex applications. External Models can vary in complexity but often leverage state-of-the-art techniques.
* Deployment: Predictive ML Models are typically maintained internally, State-of-the-Art Open Models can be deployed locally or accessed as APIs, and External Models are hosted by third parties.

### Risks in AI System Components
![3a00ceb959e87d795b1454e1e4d032bb.png](:/133039bda65b42bfb4ad53314d3c33a2)

The DASF starts with a generic AI system in terms of its constituent components and works through generic system risks. By understanding the components, how they work together Executive and the risk analysis of such architecture, an organization concerned about security can get a jump start on determining risks in its specific AI system.

* Data operations (#1-#4) include ingesting and transforming data and ensuring data security and governance. Good ML models depend on reliable data pipelines and secure DataOps infrastructure.
* Model operations (#5-#8) include building predictive ML models, acquiring models from a model marketplace, or using LLMs like OpenAI or Foundation Model APIs. Developing a model requires a series of experiments and a way to track and compare the conditions and results of those experiments.
* Model deployment and serving (#9 and #10) consists of securely building model images, isolating and securely serving models, automated scaling, rate limiting, and monitoring deployed models. Additionally, it includes feature and function serving, a high-availability, low-latency service for structured data in retrieval augmented generation (RAG) applications, as well as features that are required for other applications, such as models served outside of the platform or any other application that requires features based on data in the catalog.
* Operations and platform (#11 and #12) include platform vulnerability management and patching, model isolation and controls to the system, and authorized access to models with security in the architecture. Also included is operational tooling for CI/CD. It ensures the complete lifecycle meets the required standards by keeping the distinct execution environments — development, staging and production — for secure MLOps.

![3a228552b2578a7202c53d674cbfb04a.png](:/48651e9f6f0a458584adefac1514a6ec)
![1ab773bfe31518d4f97f4dc69d2d3ce1.png](:/978d43e16a7c4d0ea11423a06bc54f69)
![f926eed815ac509ad56d76ff0f518cb8.png](:/61cf1d929b5040fdbe79151c68c21f1c)
![e1077a31ac75e20b3413e264478aba30.png](:/5d9184130c324d51bc85cb353f16b4c3)

More informations: [https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)

### Note
This framework is aware of nascent risks such as energy-latency attacks, rowhammer attacks, side channel attacks, evasion attacks, functional adversarial attacks and other adversarial examples, but these are out of scope for this version of the framework. 

