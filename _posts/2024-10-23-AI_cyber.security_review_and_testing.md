---
layout: post
title: Comment l’IA devient une cible - Le point de vue d’un pentester
tags: [AI, Security, LLM, RAG]
description: "Comment l’IA devient une cible - Le point de vue d’un pentester"
---

# Comment l’IA devient une cible - Le point de vue d’un pentester
## Introduction
### Contexte et objectifs
Bonjour à toutes et à tous, dans un monde où l'intelligence artificielle, plus particulièrement les LLM, devient de plus en plus omniprésente, il est crucial de comprendre les vulnérabilités qu'elle peut présenter . C'est pourquoi je vais vous présenter "Comment l’IA devient une cible : Le point de vue d’un pentester".

### Avertissement : Cadre de la présentation
Avant de plonger dans le sujet, il est important de rappeler certains points pour mieux contextualiser ce qui va suivre :

1. **Recherche personnelle** : Cette présentation repose sur mes expériences et recherches personnelles.
2. **Vérité non absolue** : Les méthodologies et techniques évoquées ici ne sont pas immuables. La sécurité de l'IA évolue constamment et exige une adaptation continue.
3. **Méthodologies alternatives** : Les approches présentées ne sont qu'une partie des possibilités existantes. Il existe d'autres méthodologies et outils pour tester la sécurité de l'IA.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/warning.svg"> 
</p>

---

## Qu'est-ce que le Red Teaming ?

### Définition
Le Red Teaming est une méthode d’audit de sécurité offensive. Il vise à simuler des attaques réelles, imitant les tactiques d’un adversaire, pour identifier des failles. Dans le contexte de l'IA, cela devient critique, car les systèmes d'apprentissage automatique sont de plus en plus intégrés dans nos infrastructures critiques.

### Équilibrer les approches de sécurité
Dans le domaine de la cybersécurité, il est crucial d'adopter une approche équilibrée pour protéger efficacement les systèmes contre les diverses menaces. Deux méthodes complémentaires se distinguent : le Red Teaming et le Pentesting. Chacune de ces approches joue un rôle essentiel dans la sécurisation des infrastructures, notamment celles intégrant l'intelligence artificielle (IA).

#### L'Importance de l'équilibre
Pour garantir une sécurité optimale, il est crucial de combiner ces deux approches. Le Red Teaming permet de se préparer aux attaques sophistiquées et spécifiques à l'IA, tandis que le Pentesting assure la protection contre les vulnérabilités traditionnelles. En équilibrant ces méthodes, les organisations peuvent construire des défenses plus robustes et mieux protéger leurs systèmes contre une large gamme de menaces.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/redteam_vs_pentest.svg"> 
</p>

---

## Concepts de base
### Les LLM ne se limitent pas qu’au texte
Les modèles de langage, ne se limitent pas à la génération de texte. Ils s'appliquent à des domaines variés tels que le traitement du langage naturel (NLP), la vision par ordinateur (computer vision) pour les images, et l'audio pour les voix. Ils acceptent également d'autres formats d'entrée de données, formant ainsi des modèles multimodaux capables de traiter et croiser différents types d'informations. Cette diversité élargit considérablement leur surface d'attaque.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/multimodales.svg"> 
</p>

---

## Menaces principales et risques
### Papiers de recherche
De nombreuses recherches sortent chaque mois, dévoilant de nouvelles attaques spécifiques à l'IA. Parmi mes principales sources d'information figure [arxiv.org](arxiv.org), où des papiers récents explorent des attaques novatrices. En voici quelques-unes pour l'exemple :

- LoBAM : Ajout d'une backdoor lors de la fusion de modèles. [Lien PDF](https://arxiv.org/pdf/2411.16746)
- "Moralized" Multi-Step Jailbreak Prompts : Contournement des mécanismes de sécurité via des prompts multi-étapes. [Lien PDF](https://arxiv.org/pdf/2411.16730)
- Memory Backdoor Attacks : Extraction de données d'entraînement confidentielles. [Lien PDF](https://arxiv.org/pdf/2411.14516)
- AnywhereDoor : Backdoor flexible pour contrôler les modèles de détection d'objets. [Lien PDF](https://arxiv.org/pdf/2411.14243)

Cette veille continue est essentielle pour anticiper les menaces et adapter des stratégies de sécurité en conséquence.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/research_papers.svg"> 
</p>

### Risques spécifiques aux LLM
Le premier jet du Top 10 OWASP dédié aux vulnérabilités des LLM date de 2023. Cependant, avec les avancées rapides et les nombreuses recherches sur le sujet, le Top 10 a dû être mis à jour et celui de 2025 a évolué pour s'adapter aux nouvelles menaces et découvertes. Celui-ci comprend :

- **LLM01:2025 Prompt Injection** : Manipulation des entrées pour influencer les réponses.
- **LLM02:2025 Sensitive Information Disclosure** : Divulgation involontaire d'informations sensibles.
- **LLM03:2025 Supply Chain** : Vulnérabilités introduites via les dépendances de la chaîne d'approvisionnement.
- **LLM04:2025 Data and Model Poisoning** : Corruption des données ou du modèle d'entraînement.
- **LLM05:2025 Improper Output Handling** : Gestion inadéquate des sorties générées.
- **LLM06:2025 Excessive Agency** : Autorisations excessives menant à des actions imprévues.
- **LLM07:2025 System Prompt Leakage** : Fuite de prompts systèmes sensibles.
- **LLM08:2025 Vector and Embedding Weaknesses** : Faiblesses dans les vecteurs ou les embeddings exploités.
- **LLM09:2025 Misinformation** : Génération de fausses informations.
- **LLM10:2025 Unbounded Consumption** : Consommation excessive de ressources.

L'OWASP met en lumière les principales menaces auxquelles les LLM sont exposés, et il est essentiel, de comprendre comment ces vulnérabilités peuvent être exploitées pour causer des dommages.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/volcano.svg"> 
</p>

---

## Processus de Red Teaming
### Étapes clés
Le Red Teaming suit un processus structuré comprenant plusieurs étapes essentielles :
- **Planification et préparation** : Définition du périmètre et planification du Red Teaming pour le système d'IA cible.
- **Cartographie des risques** : Analyse des risques et développement de scénarios de risques et d'attaques.
- **Réalisation des scénarios d'attaque** : Exécution des scénarios et prise de notes détaillées.
- **Rapport** : Analyse des résultats et rédaction d'un rapport avec des recommandations claires.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/etapes_clés.svg"> 
</p>


### Modèles de LLM
Définir le type de modèles utilisés par le client est essentiel pour adapter les tests de sécurité. Parmi les catégories courantes :
- **LLM développés en interne** : modèles créés et entraînés spécifiquement par l'organisation.
- **LLM pré-entraînés avec fine-tuning par d'autres organisations** : modèles ajustés pour des besoins spécifiques après un pré-entraînement initial.
- **LLM open source** : modèles accessibles publiquement sans modifications supplémentaires.
- **LLM open source avec fine-tuning** : modèles open source adaptés pour des tâches spécifiques.
- **LLM accessibles via API externe** : solutions hébergées fournies par des tiers et utilisées à distance.

Une bonne compréhension de ces catégories permet de poser les bases pour des tests efficaces.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/configurations_client.svg"> 
</p>

### Quel Type de tests conduire ?
Les tests doivent inclure des scénarios réalistes adaptés au niveau de connaissance disponible :
- **Black-box** : Utilisé sans aucune information préalable sur le système, garantissant un test impartial.
- **Gray-box** : Combine des informations partielles pour trouver un équilibre entre précision et flexibilité.
- **White-box** : Exploite une connaissance complète pour mener des tests approfondis et exhaustifs.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/type_of_redteam.svg"> 
</p>

### Quel environnement ?
Il est essentiel de choisir l'environnement de tests avec soin :
- **Environnement de production** : Idéal pour l'application réelle, mais plus risqué en raison de l'impact potentiel sur les systèmes en direct.
- **Environnement de développement** : Adapté aux premières étapes de développement, mais peut manquer de réalisme.
- **Environnement de pré-production** : Offre un équilibre entre réalisme et sécurité, permettant des tests dans des conditions similaires à la production.

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/environments.png"> 
</p>


### Scénarios de risques et d’attaques
TODO

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/scenarios.svg"> 
</p>


### Quelle méthode utiliser pour exécuter les attaques ?
TODO

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/method_to_use.svg"> 
</p>


### Analyse des résultats et rapport
TODO

<p align="center" width="100%">
    <img src="https://raw.githubusercontent.com/Liodeus/liodeus.github.io/refs/heads/master/assets/imgs/AI/resultats.svg"> 
</p>

---

## Conclusion
### Synthèse
TODO

---

## Sources

* [AI Safety Guidelines](https://aisi.go.jp/assets/pdf/ai_safety_RT_v1.00_en.pdf)
* [Advancing Red Teaming with People and AI](https://openai.com/index/advancing-red-teaming-with-people-and-ai)
* [OWASP Top 10 for LLM](https://genai.owasp.org/llm-top-10/)
* [Schémas - Napkin.ai](https://app.napkin.ai/)
