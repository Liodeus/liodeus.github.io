---
layout: post
title: Comment l’IA devient une cible - Le point de vue d’un pentester
tags: [AI, Security, LLM, RAG]
description: "Comment l’IA devient une cible - Le point de vue d’un pentester"
---

# Comment l’IA devient une cible - Le point de vue d’un pentester
## Introduction
### Contexte et objectifs
Bonjour à toutes et à tous, dans un monde où l'intelligence artificielle, plus particulièrement les LLM, devient de plus en plus omniprésente, il est crucial de comprendre les vulnérabilités qu'elle peut présenter . C'est pourquoi je vais vous présenter "Comment l’IA devient une cible : Le point de vue d’un pentester".

### Avertissement : Cadre de la présentation
Avant de plonger dans le sujet, il est important de rappeler certains points pour mieux contextualiser ce qui va suivre :

1. **Recherche personnelle** : Cette présentation repose sur mes expériences et recherches personnelles.
2. **Vérité non absolue** : Les méthodologies et techniques évoquées ici ne sont pas immuables. La sécurité de l'IA évolue constamment et exige une adaptation continue.
3. **Méthodologies alternatives** : Les approches présentées ne sont qu'une partie des possibilités existantes. Il existe d'autres méthodologies et outils pour tester la sécurité de l'IA.

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/warning.svg"> 
</p>

---

## Qu'est-ce que le Red Teaming ?

### Définition
Le Red Teaming est une méthode d’audit de sécurité offensive. Il vise à simuler des attaques réelles, imitant les tactiques d’un adversaire, pour identifier des failles. Dans le contexte de l'IA, cela devient critique, car les systèmes d'apprentissage automatique sont de plus en plus intégrés dans nos infrastructures critiques.

### Équilibrer les approches de sécurité
Dans le domaine de la cybersécurité, il est crucial d'adopter une approche équilibrée pour protéger efficacement les systèmes contre les diverses menaces. Deux méthodes complémentaires se distinguent : le Red Teaming et le Pentesting. Chacune de ces approches joue un rôle essentiel dans la sécurisation des infrastructures, notamment celles intégrant l'intelligence artificielle (IA).

#### **L'Importance de l'équilibre**
Pour garantir une sécurité optimale, il est crucial de combiner ces deux approches. Le Red Teaming permet de se préparer aux attaques sophistiquées et spécifiques à l'IA, tandis que le Pentesting assure la protection contre les vulnérabilités traditionnelles. En équilibrant ces méthodes, les organisations peuvent construire des défenses plus robustes et mieux protéger leurs systèmes contre une large gamme de menaces.

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/redteam_vs_pentest.svg"> 
</p>

---

## Concepts de base
### Les LLM ne se limitent pas qu’au texte
Les modèles de langage, ne se limitent pas à la génération de texte. Ils s'appliquent à des domaines variés tels que le traitement du langage naturel (NLP), la vision par ordinateur (computer vision) pour les images, et l'audio pour les voix. Ils acceptent également d'autres formats d'entrée de données, formant ainsi des modèles multimodaux capables de traiter et croiser différents types d'informations. Cette diversité élargit considérablement leur surface d'attaque.

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/multimodales.svg"> 
</p>

---

## Menaces principales et risques
### Papiers de recherche
De nombreuses recherches sortent chaque mois, dévoilant de nouvelles attaques spécifiques à l'IA. Parmi mes principales sources d'information figure [arxiv.org](arxiv.org), où des papiers récents explorent des attaques novatrices. En voici quelques-unes pour l'exemple :

- LoBAM : Ajout d'une backdoor lors de la fusion de modèles. [Lien PDF](https://arxiv.org/pdf/2411.16746)
- "Moralized" Multi-Step Jailbreak Prompts : Contournement des mécanismes de sécurité via des prompts multi-étapes. [Lien PDF](https://arxiv.org/pdf/2411.16730)
- Memory Backdoor Attacks : Extraction de données d'entraînement confidentielles. [Lien PDF](https://arxiv.org/pdf/2411.14516)
- AnywhereDoor : Backdoor flexible pour contrôler les modèles de détection d'objets. [Lien PDF](https://arxiv.org/pdf/2411.14243)

Cette veille continue est essentielle pour anticiper les menaces et adapter des stratégies de sécurité en conséquence.

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/research_papers.svg"> 
</p>

### Risques spécifiques aux LLM
Le premier jet du Top 10 OWASP dédié aux vulnérabilités des LLM date de 2023. Cependant, avec les avancées rapides et les nombreuses recherches sur le sujet, le Top 10 a dû être mis à jour et celui de 2025 a évolué pour s'adapter aux nouvelles menaces et découvertes. Celui-ci comprend :

- **LLM01:2025 Prompt Injection** : Manipulation des entrées pour influencer les réponses.
- **LLM02:2025 Sensitive Information Disclosure** : Divulgation involontaire d'informations sensibles.
- **LLM03:2025 Supply Chain** : Vulnérabilités introduites via les dépendances de la chaîne d'approvisionnement.
- **LLM04:2025 Data and Model Poisoning** : Corruption des données ou du modèle d'entraînement.
- **LLM05:2025 Improper Output Handling** : Gestion inadéquate des sorties générées.
- **LLM06:2025 Excessive Agency** : Autorisations excessives menant à des actions imprévues.
- **LLM07:2025 System Prompt Leakage** : Fuite de prompts systèmes sensibles.
- **LLM08:2025 Vector and Embedding Weaknesses** : Faiblesses dans les vecteurs ou les embeddings exploités.
-** LLM09:2025 Misinformation** : Génération de fausses informations.
- **LLM10:2025 Unbounded Consumption** : Consommation excessive de ressources.

L'OWASP met en lumière les principales menaces auxquelles les LLM sont exposés, et il est essentiel, de comprendre comment ces vulnérabilités peuvent être exploitées pour causer des dommages.

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/volcano.svg"> 
</p>

---

## Processus de Red Teaming
### Étapes clés
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/etapes_clés.svg"> 
</p>


### Modèles de LLM
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/configurations_client.svg"> 
</p>

### Quel Type de tests conduire ?
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/type_of_redteam.svg"> 
</p>

### Quel environnement ?
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/environments.svg"> 
</p>


### Scénarios de risques et d’attaques
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/scenarios.svg"> 
</p>


### Quelle méthode utiliser pour exécuter les attaques ?
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/method_to_use.svg"> 
</p>


### Analyse des résultats et rapport
TODO

<p align="center" width="100%">
    <img src="https://github.com/Liodeus/liodeus.github.io/blob/master/assets/imgs/AI/resultats.svg"> 
</p>

---

## Conclusion
### Synthèse
TODO

---

## Sources

* [AI Safety Guidelines](https://aisi.go.jp/assets/pdf/ai_safety_RT_v1.00_en.pdf)
* [Advancing Red Teaming with People and AI](https://openai.com/index/advancing-red-teaming-with-people-and-ai)
* [OWASP Top 10 for LLM](https://genai.owasp.org/llm-top-10/)
* [Schémas - Napkin.ai](https://app.napkin.ai/)
