---
layout: post
title: AI cybersecurity review and testing
tags: [AI, Security, LLM, RAG]
description: "AI cybersecurity review and testing"
---

- [AI cybersecurity review and testing](#ai-cybersecurity-review-and-testing)
   * [Methodology](#methodology)
      + [Client objectives and scope definition  ](#client-objectives-and-scope-definition)
      + [Attack surface identification](#attack-surface-identification)
   * [Chain of thought and data gathering](#chain-of-thought-and-data-gathering)
      + [Databricks AI security framework (DSAF)](#databricks-ai-security-framework-dsaf)
         - [Model types](#model-types)
         - [Risks in AI System Components](#risks-in-ai-system-components)
         - [Model deployment and serving](#model-deployment-and-serving)

---

# AI cybersecurity review and testing
The goal of this guide is to provide a comprehensive overview of AI penetration testing from a pentester's perspective, detailing the entire process—from initial client interactions to the specific tests required.

All sources used to create this article are listed in the [Sources](#sources) section and are explained throughout this article.

I will focus on a black-box attacker perspective, with the possibility of incorporating some greybox elements depending on the client's input.

## Methodology
### Client objectives and scope definition
In this phase, clarify the client's goals and establish the scope of the pentest. The client may specify particular AI components, APIs, or systems to focus on. Review the client's objectives to ensure alignment, and clearly define the boundaries of the testing process, including any restrictions or areas considered out of scope. This ensures that the testing efforts are targeted and meet the client's expectations.

**Questions to Ask**:
* What type of AI models are being used? (Predictive ML Models, State-of-the-Art Open Models, External Models - [Cf. Model types](#model-types))
* What is the goal of the AI system? (e.g., prediction, natural language generation, image classification)
* What external interfaces (APIs, web apps) are used to interact with the model?
* Are there third-party services involved?
* Are there any rate-limiting or security controls on the APIs?
* What frameworks and libraries were used in developing the AI model (e.g., TensorFlow, PyTorch)?

### Attack surface identification
In a black-box/gray-box pentest, the client may provide credentials, documentation about the AI system and/or API, schema/diagra m, etc. Review all the materials provided to gain a clearer understanding of the tests you’ll need to conduct. Explore the application and map out potential vulnerability areas (mostly experience based).

**<u>Key Steps:</u>**
* Review client materials: Go through all documents provided by the client to understand the general system design, including data flows, APIs, and security mechanisms. This helps establish a baseline understanding of the environment.
* Map external interfaces: Identify the points where the AI system interacts with external entities, such as APIs, web applications, or third-party services. Focus on documenting how these interfaces are exposed and what potential risks might be associated with them.
* Examine AI model exposure: Understand how the AI model accepts inputs, especially from external sources. Capture details about the types of input it processes and any interactions it has with other systems or users.
* Analyze data flow: Trace the movement of data through the system, from ingestion to processing, storage, and output. Document how data moves between different components and note where sensitive data may be involved.
* Identify components and dependencies: Break down the system into its core components, such as databases, AI models, APIs, and third-party dependencies. Map out which components may interact with others, and take note of external libraries or services in use.
* Assess API and user security: Identify the different roles users can take and how they interact with the system. Document the APIs and user interfaces, focusing on who can access them and how.
* Cloud/On-Premise considerations: Record details about the environment where the system is deployed, whether cloud-based or on-premise. Note key configurations that might affect the security of the deployment, such as network access or infrastructure details.
* Threat modeling: Based on the collected information, start outlining potential areas of risk, that can be explored further during the pentest.

![Decision_tree](/assets/imgs/AI_security/tree.png)

## Chain of thought and data gathering
### Databricks AI security framework (DSAF)
#### Model types
First, find out what type of AI models are being built or being used:
1. **<u>Predictive ML Models</u>**: These models focus on traditional machine learning techniques and are specifically designed for structured data. They are trained on your enterprise’s tabular datasets and are typically implemented in Python, often packaged in the MLflow format for deployment. Examples of predictive ML models include scikit-learn, XGBoost, PyTorch, and Hugging Face transformer models. Their primary use is to perform tasks such as regression and classification based on historical data.
2. **<u>State-of-the-Art Open Models</u>**: This category encompasses advanced foundation models that are designed for optimized inference on a variety of tasks, particularly those involving unstructured data. Examples include Meta Llama 3.1-405B-Instruct, BGE-Large, and Mixtral-8x7B, which are available for immediate use with pay-per-token pricing. These models can be deployed for workloads that require performance guarantees and can be fine-tuned for specific applications. Their usage patterns include Foundation Model APIs for large language models (LLMs), retrieval-augmented generation (RAG), pretraining, and fine-tuning, making them suitable for complex applications like natural language processing.
3. **<u>External Models (Third-Party Services)</u>**: These models are hosted outside of your organization, typically provided by third-party vendors. Endpoints that serve external models can be centrally governed, allowing organizations to set rate limits and access controls. Examples include widely used foundation models such as OpenAI’s GPT-4 and Anthropic’s Claude. While these models provide easy access to advanced capabilities, they require reliance on the provider’s infrastructure and policies.

<u>Summary of differences</u>:
* **<u>Data type</u>**: Predictive ML Models focus on structured data, while State-of-the-Art Open Models are suited for unstructured data. External Models can cover both but are accessed via third-party services.
* **<u>Complexity</u>**: Predictive ML Models are generally simpler, targeting specific prediction tasks, whereas State-of-the-Art Open Models use advanced architectures for complex applications. External Models can vary in complexity but often leverage state-of-the-art techniques.
* **<u>Deployment</u>**: Predictive ML Models are typically maintained internally, State-of-the-Art Open Models can be deployed locally or accessed as APIs, and External Models are hosted by third parties.

### Risks in AI System Components
![DASF schema](/assets/imgs/AI_security/dasf_schema.png)

The DASF starts with a generic AI system in terms of its constituent components and works through generic system risks. By understanding the components, how they work together Executive and the risk analysis of such architecture, an organization concerned about security can get a jump start on determining risks in its specific AI system.

* Data operations (**1-4**) include ingesting and transforming data and ensuring data security and governance. Good ML models depend on reliable data pipelines and secure DataOps infrastructure.
* Model operations (**5-8**) include building predictive ML models, acquiring models from a model marketplace, or using LLMs like OpenAI or Foundation Model APIs. Developing a model requires a series of experiments and a way to track and compare the conditions and results of those experiments.
* Model deployment and serving (**9 and 10**) consists of securely building model images, isolating and securely serving models, automated scaling, rate limiting, and monitoring deployed models. Additionally, it includes feature and function serving, a high-availability, low-latency service for structured data in retrieval augmented generation (RAG) applications, as well as features that are required for other applications, such as models served outside of the platform or any other application that requires features based on data in the catalog.
* Operations and platform (**11 and 12**) include platform vulnerability management and patching, model isolation and controls to the system, and authorized access to models with security in the architecture. Also included is operational tooling for CI/CD. It ensures the complete lifecycle meets the required standards by keeping the distinct execution environments — development, staging and production — for secure MLOps.

They have identified **55** technical security risks across the **12** components based on the AI model types:
![Data operations](/assets/imgs/AI_security/data_operations.png)
![Model operations](/assets/imgs/AI_security/model_operations.png)
![Model deployment and serving](/assets/imgs/AI_security/model_deployment_and_serving.png)
![Operations and platform](/assets/imgs/AI_security/operations_and_platform.png)

**Note**: The authors of the framework are aware of emerging risks such as energy-latency attacks, Rowhammer attacks, side-channel attacks, evasion attacks, functional adversarial attacks, and other adversarial examples. However, these are outside the scope of this version of the framework. They may reconsider these and any new novel risks in later versions if they see them becoming material.

#### Model deployment and serving
Since the focus is on a black-box/grey-box penetration testing perspective, I'll review this section as it is the most relevant. I attempted to align each of them with the [top 10 for LLM by the OWASP](https://genai.owasp.org/llm-top-10/):

<u>**Model Serving - inference requests**</u>
* Prompt inject - LLM01
* Model inversion - LLM10
* Model breakout - LLM01
* Looped input - LLM09
* Infer training data membership - LLM10
* Discover ML model ontology - LLM10
* Denial of service (DOS) - LLM04
* LLM hallucinations - LLM09
* Input resource control - LLM01
* Accidental exposure of unauthorized data to models - LLM06

<u>**Model Serving - inference responses**</u>
* Lack of audit and monitring inference quality - None
* Output manipulation - LLM06
* Discover ML model ontology - LLM10
* Discover ML model family - LLM10
* Black-box attacks - None

The document is somewhat vague in certain areas, but I will still use parts of it later to develop a testing methodology.

## Sources
* [DASF](https://www.databricks.com/resources/whitepaper/databricks-ai-security-framework-dasf)
* [top 10 for LLM by the OWASP](https://genai.owasp.org/llm-top-10/)
* [Nvidia - practical LLM Security](https://i.blackhat.com/BH-US-24/Presentations/US24-Harang-Practical-LLM-Security-Takeaways-From-Wednesday.pdf)
